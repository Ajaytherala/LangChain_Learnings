{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qq -U langchain-openai langchain-core langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import  RunnableLambda, RunnableBranch\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(\n",
    "    model_name = \"gpt-4o-mini\",\n",
    "    temperature = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [\n",
    "    \"what is the capital of Japan?\", #question - answer\n",
    "    \"Once upon a time in a faraway land, there was a brave knight who fought dragons to protect his kingdom.\", #story - summarize\n",
    "    \"Technology is evolving rapidly, and AI is shaping the future of work . \", #statement - generate title\n",
    "    \"self-driving cars will change the way we travel.\" #statement - generate title\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_prompt  = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content = \"You are an AI model. Your work is to classify whether the provided input text is either question or statement or story and return the category only\"),\n",
    "    (\"human\", \"Classify the text :\\n\\n {text} \\n category: \")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_step = classification_prompt | chat_model | (lambda x:{\"category\":x.content.strip().lower()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_prompt  = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content = \"You are an AI model. Your work is to answer the question accurately and reply with I don't know if you aren't aware of answer\"),\n",
    "    (\"human\", \"Answer the question :\\n\\n {text} \")\n",
    "])\n",
    "summary_prompt  = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content = \"You are an AI model. Your work is to summarize the provided text\"),\n",
    "    (\"human\", \"summarize the text :\\n\\n {text}\")\n",
    "])\n",
    "title_prompt  = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content = \"You are an AI model. Your work is to provide the suitable title for the statement provided.\"),\n",
    "    (\"human\", \"provide the title that aptly suits for the following text :\\n\\n {text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question = answer_prompt | chat_model | (lambda x : {\"response\" : x.content.strip()})\n",
    "summarize_story = summary_prompt | chat_model | (lambda x : {\"response\" : x.content.strip()})\n",
    "generate_title = title_prompt | chat_model | (lambda x : {\"response\" : x.content.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    RunnableLambda(lambda x : {\"text\" : x}) | \n",
    "    (lambda x : {**x, **classification_step.invoke(x)}) |\n",
    "    RunnableBranch(\n",
    "        (lambda x : x[\"category\"] == \"question\", answer_question),\n",
    "        (lambda x : x[\"category\"] == \"story\", summarize_story),\n",
    "        (generate_title)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'The capital of Japan is Tokyo.'}\n",
      "{'response': 'In a distant land, a courageous knight battled dragons to safeguard his kingdom.'}\n",
      "{'response': '\"Embracing the Future: The Impact of AI on the Evolving Workplace\"'}\n",
      "{'response': '\"Revolutionizing Travel: The Impact of Self-Driving Cars\"'}\n"
     ]
    }
   ],
   "source": [
    "for message in input_messages:\n",
    "    response = pipeline.invoke(message)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
